Implement Linear Regression
----------------------------
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
#from sklearn.linear_model import LinearRegression 
from numpy.linalg import pinv

# df for dataframe, s for series
df = pd.read_csv('C:\\Users\\MCA_Dept_TKM_2019\\Desktop\\EnergyPrediction\\Dataset2009.csv')
df['Date'] = pd.to_datetime(df['Date'])
# total power consumption
s_power_consumption = df.groupby('Date')['Demand met'].sum()
#s_power_consumption.index = pd.to_datetime(s_power_consumption.index).sort_values()

def corr_xy_plot(s, k):
   
    n = len(s)
    for i in range(0,n-k):
        plt.scatter(s[i],s[i+k])
    plt.title("lag=%s"%k)
    plt.show()
#corr_xy_plot(s_power_consumption.values,1)
#corr_xy_plot(s_power_consumption.values,3)
#corr_xy_plot(s_power_consumption.values,5)
#corr_xy_plot(s_power_consumption.values,7)
#corr_xy_plot(s_power_consumption.values,9)

def corr_fit_plot(s, k):
    
    n = len(s)
    x = []; y = []
    for i in range(0,n-k):
        x.append([s[i]])
        y.append([s[i+k]])
    plt.scatter(x,y)
    
#    # using sklearn
#    re = LinearRegression()
#    re.fit(x,y)
#    pred = re.predict(x)
#    coef = re.coef_
#    plt.plot(x,pred,'r-')
    
    # least square by myself
    x = np.array(x)
    y = np.array(y)
    one = np.ones((x.shape[0],1))
    x = np.concatenate((one,np.array(x)),axis=1)
    coefs = np.dot(pinv(x),y)
    pred = coefs[0]*x[:,0] + coefs[1]*x[:,1]
    coef = coefs[1]
    plt.plot(x[:,1],pred,'r-')
    
    plt.title('Corr=%s'%coef+' Lag=%s'%k)
    plt.show()    

    return coef

#corr_fit_plot(s_power_consumption.values,1)
#corr_fit_plot(s_power_consumption.values,3)
#corr_fit_plot(s_power_consumption.values,5)
#corr_fit_plot(s_power_consumption.values,7)
corr_fit_plot(s_power_consumption.values,60)

plt.hist(s_power_consumption.values,bins=50)
plt.show()


def auto_corr(x, l):
    return [x.autocorr(i) for i in l]

corr = auto_corr(s_power_consumption,range(1,60))
plt.stem(corr)
plt.title('Auto Correlation')
plt.show()

def partial_corr(x, k):
    
    n = len(x)
    X = [];Y=[]
    for i in range(0,n-k-1):
        X.append(x[i:i+k])
        Y.append(x[i+k+1])
    X = np.array(X); Y = np.array(Y)
    one = np.ones((X.shape[0],1))
    X = np.concatenate((one,X), axis=1)
    coef = np.dot(pinv(X),Y)
#    print 'coef=%s'%coef
    return coef[1] 
       
def partial_corrs(x, lag=10):
    coefs = []
    for i in range(1,lag+1):
        coefs.append(partial_corr(x, i))
    return coefs

partial_coefs = partial_corrs(s_power_consumption.values, lag=200)
plt.stem(partial_coefs)
plt.title('Partial Auto Correlation')
plt.show()


#def seq2_regression(first=0,second=1,length=50):
#    auto = [0]*length;
#    auto[0]=first;auto[1]=second
#    for i in xrange(2,length):
#        auto[i] = 0.6*auto[i-1] + 0.2*auto[i-2] + np.random.randn()
#    return auto
#
#x = seq2_regression(length = 200)
#
#corr = auto_corr(pd.Series(data=x),range(1,60))
#plt.stem(corr)
#plt.title('Auto Correlation')
#plt.show()
#
#partial_coefs = partial_corrs(x, lag=30)
#plt.stem(partial_coefs)
#plt.title('Partial Auto Correlation')
#plt.show()
#

#s1 = pd.Series(data = s_power_consumption.values[0:-1])
#s2 = pd.Series(data = s_power_consumption.values[1:])
#delta_power = s2-s1
#
#corr = auto_corr(delta_power,range(1,60))
#plt.stem(corr)
#plt.title('Auto Correlation')
#plt.show()
#
#partial_coefs = partial_corrs(delta_power.values, lag=30)
#plt.stem(partial_coefs)
#plt.title('Partial Auto Correlation')
#plt.show()

from statsmodels.tsa.seasonal import seasonal_decompose
decomposition = seasonal_decompose(s_power_consumption.values,freq=14)

trend = decomposition.trend
seasonal = decomposition.seasonal
residual = decomposition.resid


plt.rc("figure", figsize=(25, 10))
plt.subplot(411)
plt.plot(s_power_consumption.values, label='Original')
plt.legend(loc='best')
plt.subplot(412)
plt.plot(trend, label='Trend')
plt.legend(loc='best')
plt.subplot(413)
plt.plot(seasonal,label='Seasonality')
plt.legend(loc='best')
plt.subplot(414)
plt.plot(residual, label='Residuals')
plt.legend(loc='best')
plt.tight_layout()
plt.show()

partial_coefs = partial_corrs(residual[7:-8], lag=200)
plt.stem(partial_coefs)
plt.title('Partial Auto Correlation')
plt.show()

def corr(s, k):
    
    n = len(s)
    x = []; y = []
    for i in range(0,n-k):
        x.append([s[i]])
        y.append([s[i+k]])
    
    # least square by myself
    x = np.array(x)
    y = np.array(y)
    one = np.ones((x.shape[0],1))
    x = np.concatenate((one,np.array(x)),axis=1)
    coefs = np.dot(pinv(x),y)
    coef = coefs[1]

    return coef

def auto_corr(s, lags):
    return np.array([corr(s, k) for k in lags])

corr = auto_corr(residual[7:-8],range(1,200))
plt.stem(corr)
plt.title('Auto Correlation')
plt.show()
---------Implement Random Forest------------
--------------------------------------------
#Random Forest Algorithm on Engery Dataset
from random import seed
from random import randrange
from csv import reader
from math import sqrt

#Load a CSV File
def load_csv(filename):
    dataset=list()
    with open(filename,'r') as file:
        csv_reader=reader(file)
        for row in csv_reader:
            if not row:
                continue
            dataset.append(row)
    return dataset

#Convert string column to float
def str_column_to_float(dataset,column):
    for row in dataset:
        row[column]=float(row[column].strip())

#Convert string column to integer
def str_column_to_int(dataset,column):
    class_values=[row[column] for row in dataset]
    unique=set(class_values)
    lookup=dict()
    for i, value in enumerate(unique):
        lookup[value]=i
    for row in dataset:
        row[column]=lookup[row[column]]
    return lookup

#Split a dataset into k folds
def cross_validation_split(dataset,n_folds):
    dataset_split=list()
    dataset_copy=list(dataset)
    fold_size=int(len(dataset)/n_folds)
    for i in range(n_folds):
        fold=list()
        while len(fold)<fold_size:
            index=randrange(len(dataset_copy))
            fold.append(dataset_copy.pop(index))
        dataset_split.append(fold)
    return dataset_split

#Calculate accuracy percentage
def accuracy_metric(actual,predicted):
    correct=0
    for i in range(len(actual)):
        if actual[i]==predicted[i]:
            correct+=1
    return correct/float(len(actual))*100.0

#Evaluate an algorithm using a cross validation split
def evaluate_algorithm(dataset,algorithm,n_folds,*args):
    folds=cross_validation_split(dataset,n_folds)
    scores=list()
    for fold in folds:
        train_set=list(folds)
        train_set.remove(fold)
        train_set=sum(train_set,[])
        test_set=list()
        for row in fold:
            row_copy=list(row)
            test_set.append(row_copy)
            row_copy[-1]=None
        predicted=algorithm(train_set,test_set,*args)
        actual=[row[-1] for row in fold]
        accuracy=accuracy_metric(actual,predicted)
        scores.append(accuracy)
    return scores
#Split a dataset based on attribute and an attribute value
def test_split(index,value,dataset):
    left,right=list(),list()
    for row in dataset:
        if row[index]<value:
            left.append(row)
        else:
            right.append(row)
    return left,right

#Calculate the Gini index for a split dataset
def gini_index(groups,class_values):
    gini=0.0
    for class_value in class_values:
        for group in groups:
            size=len(group)
            if size==0:
                continue
            proportion=[row[-1] for row in group].count(class_value)/float(size)
            gini+=(proportion*(1.0-proportion))
    return gini

# Select the best split point for a dataset
def get_split(dataset, n_features):
	class_values = list(set(row[-1] for row in dataset))
	b_index, b_value, b_score, b_groups = 999, 999, 999, None
	features = list()
	while len(features) < n_features:
		index = randrange(len(dataset[0])-1)
		if index not in features:
			features.append(index)
	for index in features:
		for row in dataset:
			groups = test_split(index, row[index], dataset)
			gini = gini_index(groups, class_values)
			if gini < b_score:
				b_index, b_value, b_score, b_groups = index, row[index], gini, groups
	return {'index':b_index, 'value':b_value, 'groups':b_groups}
 
# Create a terminal node value
def to_terminal(group):
	outcomes = [row[-1] for row in group]
	return max(set(outcomes), key=outcomes.count)
 
# Create child splits for a node or make terminal
def split(node, max_depth, min_size, n_features, depth):
	left, right = node['groups']
	del(node['groups'])
	# check for a no split
	if not left or not right:
		node['left'] = node['right'] = to_terminal(left + right)
		return
	# check for max depth
	if depth >= max_depth:
		node['left'], node['right'] = to_terminal(left), to_terminal(right)
		return
	# process left child
	if len(left) <= min_size:
		node['left'] = to_terminal(left)
	else:
		node['left'] = get_split(left, n_features)
		split(node['left'], max_depth, min_size, n_features, depth+1)
	# process right child
	if len(right) <= min_size:
		node['right'] = to_terminal(right)
	else:
		node['right'] = get_split(right, n_features)
		split(node['right'], max_depth, min_size, n_features, depth+1)
 
# Build a decision tree
def build_tree(train, max_depth, min_size, n_features):
	root = get_split(train, n_features)
	split(root, max_depth, min_size, n_features, 1)
	return root
 
# Make a prediction with a decision tree
def predict(node, row):
	if row[node['index']] < node['value']:
		if isinstance(node['left'], dict):
			return predict(node['left'], row)
		else:
			return node['left']
	else:
		if isinstance(node['right'], dict):
			return predict(node['right'], row)
		else:
			return node['right']
 
# Create a random subsample from the dataset with replacement
def subsample(dataset, ratio):
	sample = list()
	n_sample = round(len(dataset) * ratio)
	while len(sample) < n_sample:
		index = randrange(len(dataset))
		sample.append(dataset[index])
	return sample
 
# Make a prediction with a list of bagged trees
def bagging_predict(trees, row):
	predictions = [predict(tree, row) for tree in trees]
	return max(set(predictions), key=predictions.count)

#Random Forest Algorithm
def random_forest(train,test,max_depth,min_size,sample_size,n_trees,n_features):
    trees=list()
    for i in range(n_trees):
        sample=subsample(train,sample_size)
        tree=build_tree(sample,max_depth,min_size,n_features)
        trees.append(tree)
    predictions=[bagging_predict(trees,row) for row in test]
    return(predictions)

# Test the random forest algorithm
seed(1)
#load and prepare data
filename='C:\\Users\\MCA_Dept_TKM_2019\\Desktop\\EnergyPrediction\\Dataset2009.csv'
dataset=load_csv(filename)
#convert string attributes to integers
for i in range(31,len(dataset[0])-1):
    str_column_to_float(dataset,i)
#convert class column to integers
str_column_to_int(dataset,len(dataset[0])-1)
#evaluate algorithm
n_folds=5
max_depth=10
min_size=1
sample_size=1.0
n_features=int(sqrt(len(dataset[0])-1))
for n_trees in [1,5,10]:
    scores=evaluate_algorithm(dataset,random_forest,n_folds,max_depth,min_size,sample_size,n_trees,n_features)
    print('Trees:%d' % n_trees)
    print('Scores:%s' % scores)
    print('Mean Accuracy:%.3f%%'%(sum(scores)/float(len(scores))))


